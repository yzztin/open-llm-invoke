## 1. 介绍

在调用 openai 兼容接口时，工具调用和流式响应同时实现有些麻烦，原因在于：
1. 模型识别到工具调用时的响应也是流式的，需要拼接后获取到完整的工具函数请求参数
2. 如果模型识别到不使用工具，此时的响应结果是正常的流式回答，这就要求做到可以同时处理这两种情况

本仓库代码的解决思路是，在处理流式响应时，先通过生成器对象的第一个元素来判断是否是工具调用，随后再依据不同情况处理完整的生成器对象。

具体代码实现位置是：`engines/llm/openai_llm_invoke.py`，主要实现了：
- 流式和非流式模型响应
- 流式和非流市工具调用模型响应
- 中断模型输出，注意，这只是在代码调用层面停止了输出，仍然会消耗应该被消耗的 token
- 格式化模型的 json 格式回答，返回 dict 类型数据（prompt 中需要明确模型以 json 格式输出结果）

在 `tests` 目录下可以看到通过测试用例实现的使用示例

## 2. 目录结构说明
```
├── engines
│   ├── llm
│   │   ├── openai_llm_invoke.py  # 模型调用实现
│   │   └── prompts  # prompt 文本
│   │       └── static
│   └── tools
│       ├── tool_functions  # 调用工具函数入口
│       └── tool_services  # 工具函数能力实现
│           └── web_search
└── tests  # 测试用例
```